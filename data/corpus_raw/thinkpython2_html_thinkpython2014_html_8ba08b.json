{
  "title": "Chapter 13  Case study: data structure selection",
  "content": "Buy this book at Amazon.com Chapter 13  Case study: data structure selection At this point you have learned about Python’s core data structures,\nand you have seen some of the algorithms that use them.\nIf you would like to know more about algorithms, this might be a good\ntime to read Chapter B .\nBut you don’t have to read it before you go on; you can read\nit whenever you are interested. This chapter presents a case study with exercises that let\nyou think about choosing data structures and practice using them. 13.1  Word frequency analysis As usual, you should at least attempt the exercises\nbefore you read my solutions. Exercise 1 Write a program that reads a file, breaks each line into\nwords, strips whitespace and punctuation from the words, and\nconverts them to lowercase. Hint: The string module provides a string named whitespace ,\nwhich contains space, tab, newline, etc., and punctuation which contains the punctuation characters. Let’s see\nif we can make Python swear: >>> import string\n>>> string.punctuation\n'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~' Also, you might consider using the string methods strip , replace and translate . Exercise 2 Go to Project Gutenberg ( http://gutenberg.org ) and download \nyour favorite out-of-copyright book in plain text format. Modify your program from the previous exercise to read the book\nyou downloaded, skip over the header information at the beginning\nof the file, and process the rest of the words as before. Then modify the program to count the total number of words in\nthe book, and the number of times each word is used. Print the number of different words used in the book. Compare\ndifferent books by different authors, written in different eras.\nWhich author uses the most extensive vocabulary? Exercise 3 Modify the program from the previous exercise to print the\n20 most frequently used words in the book. Exercise 4 Modify the previous program to read a word list (see\nSection 9.1 ) and then print all the words in the book that\nare not in the word list. How many of them are typos? How many of\nthem are common words that should be in the word list, and how\nmany of them are really obscure? 13.2  Random numbers Given the same inputs, most computer programs generate the same\noutputs every time, so they are said to be deterministic .\nDeterminism is usually a good thing, since we expect the same\ncalculation to yield the same result. For some applications, though,\nwe want the computer to be unpredictable. Games are an obvious\nexample, but there are more. Making a program truly nondeterministic turns out to be difficult,\nbut there are ways to make it at least seem nondeterministic. One of\nthem is to use algorithms that generate pseudorandom numbers.\nPseudorandom numbers are not truly random because they are generated\nby a deterministic computation, but just by looking at the numbers it\nis all but impossible to distinguish them from random. The random module provides functions that generate\npseudorandom numbers (which I will simply call “random” from\nhere on). The function random returns a random float\nbetween 0.0 and 1.0 (including 0.0 but not 1.0). Each time you\ncall random , you get the next number in a long series. To see a\nsample, run this loop: import random\n\nfor i in range(10):\n    x = random.random()\n    print(x) The function randint takes parameters low and high and returns an integer between low and high (including both). >>> random.randint(5, 10)\n5\n>>> random.randint(5, 10)\n9 To choose an element from a sequence at random, you can use choice : >>> t = [1, 2, 3]\n>>> random.choice(t)\n2\n>>> random.choice(t)\n3 The random module also provides functions to generate\nrandom values from continuous distributions including\nGaussian, exponential, gamma, and a few more. Exercise 5 Write a function named choose_from_hist that takes\na histogram as defined in Section 11.2 and returns a \nrandom value from the histogram, chosen with probability\nin proportion to frequency. For example, for this histogram: >>> t = ['a', 'a', 'b']\n>>> hist = histogram(t)\n>>> hist\n{'a': 2, 'b': 1} your function should return 'a' with probability 2/3 and 'b' with probability 1/3 . 13.3  Word histogram You should attempt the previous exercises before you go on.\nYou can download my solution from https://thinkpython.com/code/analyze_book1.py . You will\nalso need https://thinkpython.com/code/emma.txt . Here is a program that reads a file and builds a histogram of the\nwords in the file: import string\n\ndef process_file(filename):\n    hist = dict()\n    fp = open(filename)\n    for line in fp:\n        process_line(line, hist)\n    return hist\n\ndef process_line(line, hist):\n    line = line.replace('-', ' ')\n    \n    for word in line.split():\n        word = word.strip(string.punctuation + string.whitespace)\n        word = word.lower()\n        hist[word] = hist.get(word, 0) + 1\n\nhist = process_file('emma.txt') This program reads emma.txt , which contains the text of Emma by Jane Austen. process_file loops through the lines of the file,\npassing them one at a time to process_line . The histogram hist is being used as an accumulator. process_line uses the string method replace to replace\nhyphens with spaces before using split to break the line into a\nlist of strings. It traverses the list of words and uses strip and lower to remove punctuation and convert to lower case. (It\nis a shorthand to say that strings are “converted”; remember that\nstrings are immutable, so methods like strip and lower return new strings.) Finally, process_line updates the histogram by creating a new\nitem or incrementing an existing one. To count the total number of words in the file, we can add up\nthe frequencies in the histogram: def total_words(hist):\n    return sum(hist.values()) The number of different words is just the number of items in\nthe dictionary: def different_words(hist):\n    return len(hist) Here is some code to print the results: print('Total number of words:', total_words(hist))\nprint('Number of different words:', different_words(hist)) And the results: Total number of words: 161080\nNumber of different words: 7214 13.4  Most common words To find the most common words, we can make a list of tuples,\nwhere each tuple contains a word and its frequency,\nand sort it. The following function takes a histogram and returns a list of\nword-frequency tuples: def most_common(hist):\n    t = []\n    for key, value in hist.items():\n        t.append((value, key))\n\n    t.sort(reverse=True)\n    return t In each tuple, the frequency appears first, so the resulting list is\nsorted by frequency. Here is a loop that prints the ten most common\nwords: t = most_common(hist)\nprint('The most common words are:')\nfor freq, word in t[:10]:\n    print(word, freq, sep='\\t') I use the keyword argument sep to tell print to use a tab\ncharacter as a “separator”, rather than a space, so the second\ncolumn is lined up. Here are the results from Emma : The most common words are:\nto      5242\nthe     5205\nand     4897\nof      4295\ni       3191\na       3130\nit      2529\nher     2483\nwas     2400\nshe     2364 This code can be simplified using the key parameter of\nthe sort function. If you are curious, you can read about it\nat https://wiki.python.org/moin/HowTo/Sorting . 13.5  Optional parameters We have seen built-in functions and methods that take optional\narguments. It is possible to write programmer-defined functions\nwith optional arguments, too. For example, here is a function that\nprints the most common words in a histogram def print_most_common(hist, num=10):\n    t = most_common(hist)\n    print('The most common words are:')\n    for freq, word in t[:num]:\n        print(word, freq, sep='\\t') The first parameter is required; the second is optional.\nThe default value of num is 10. If you only provide one argument: print_most_common(hist) num gets the default value. If you provide two arguments: print_most_common(hist, 20) num gets the value of the argument instead. In other\nwords, the optional argument overrides the default value. If a function has both required and optional parameters, all\nthe required parameters have to come first, followed by the\noptional ones. 13.6  Dictionary subtraction Finding the words from the book that are not in the word list\nfrom words.txt is a problem you might recognize as set\nsubtraction; that is, we want to find all the words from one\nset (the words in the book) that are not in the other (the\nwords in the list). subtract takes dictionaries d1 and d2 and returns a\nnew dictionary that contains all the keys from d1 that are not\nin d2 . Since we don’t really care about the values, we\nset them all to None. def subtract(d1, d2):\n    res = dict()\n    for key in d1:\n        if key not in d2:\n            res[key] = None\n    return res To find the words in the book that are not in words.txt ,\nwe can use process_file to build a histogram for words.txt , and then subtract: words = process_file('words.txt')\ndiff = subtract(hist, words)\n\nprint(\"Words in the book that aren't in the word list:\")\nfor word in diff:\n    print(word, end=' ') Here are some of the results from Emma : Words in the book that aren't in the word list:\nrencontre jane's blanche woodhouses disingenuousness \nfriend's venice apartment ... Some of these words are names and possessives. Others, like\n“rencontre”, are no longer in common use. But a few are common\nwords that should really be in the list! Exercise 6 Python provides a data structure called set that provides many\ncommon set operations. You can read about them in Section 19.5 ,\nor read the documentation at http://docs.python.org/3/library/stdtypes.html#types-set . Write a program that uses set subtraction to find words in the book\nthat are not in the word list. Solution: https://thinkpython.com/code/analyze_book2.py . 13.7  Random words To choose a random word from the histogram, the simplest algorithm\nis to build a list with multiple copies of each word, according\nto the observed frequency, and then choose from the list: def random_word(h):\n    t = []\n    for word, freq in h.items():\n        t.extend([word] * freq)\n\n    return random.choice(t) The expression [word] * freq creates a list with freq copies of the string word . The extend method is similar to append except that the argument is\na sequence. This algorithm works, but it is not very efficient; each time you\nchoose a random word, it rebuilds the list, which is as big as\nthe original book. An obvious improvement is to build the list\nonce and then make multiple selections, but the list is still big. An alternative is: Use keys to get a list of the words in the book. Build a list that contains the cumulative sum of the word\nfrequencies (see Exercise 2 ). The last item\nin this list is the total number of words in the book, n . Choose a random number from 1 to n . Use a bisection search\n(See Exercise 10 ) to find the index where the random\nnumber would be inserted in the cumulative sum. Use the index to find the corresponding word in the word list. Exercise 7 Write a program that uses this algorithm to choose a random word from\nthe book. Solution: https://thinkpython.com/code/analyze_book3.py . 13.8  Markov analysis If you choose words from the book at random, you can get a\nsense of the vocabulary, but you probably won’t get a sentence: this the small regard harriet which knightley's it most things A series of random words seldom makes sense because there\nis no relationship between successive words. For example, in\na real sentence you would expect an article like “the” to\nbe followed by an adjective or a noun, and probably not a verb\nor adverb. One way to measure these kinds of relationships is Markov\nanalysis, which\ncharacterizes, for a given sequence of words, the probability of the\nwords that might come next. For example, the song Eric, the Half a\nBee begins: Half a bee, philosophically, Must, ipso facto, half not be. But half the bee has got to be Vis a vis, its entity. D’you see? But can a bee be said to be Or not to be an entire bee When half the bee is not a bee Due to some ancient injury? In this text,\nthe phrase “half the” is always followed by the word “bee”,\nbut the phrase “the bee” might be followed by either\n“has” or “is”. The result of Markov analysis is a mapping from each prefix\n(like “half the” and “the bee”) to all possible suffixes\n(like “has” and “is”). Given this mapping, you can generate a random text by\nstarting with any prefix and choosing at random from the\npossible suffixes. Next, you can combine the end of the\nprefix and the new suffix to form the next prefix, and repeat. For example, if you start with the prefix “Half a”, then the\nnext word has to be “bee”, because the prefix only appears\nonce in the text. The next prefix is “a bee”, so the\nnext suffix might be “philosophically”, “be” or “due”. In this example the length of the prefix is always two, but\nyou can do Markov analysis with any prefix length. Exercise 8 Markov analysis: Write a program to read a text from a file and perform Markov\nanalysis. The result should be a dictionary that maps from\nprefixes to a collection of possible suffixes. The collection\nmight be a list, tuple, or dictionary; it is up to you to make\nan appropriate choice. You can test your program with prefix\nlength two, but you should write the program in a way that makes\nit easy to try other lengths. Add a function to the previous program to generate random text\nbased on the Markov analysis. Here is an example from Emma with prefix length 2: He was very clever, be it sweetness or be angry, ashamed or only\namused, at such a stroke. She had never thought of Hannah till you\nwere never meant for me?\" \"I cannot make speeches, Emma:\" he soon cut\nit all himself. For this example, I left the punctuation attached to the words.\nThe result is almost syntactically correct, but not quite.\nSemantically, it almost makes sense, but not quite. What happens if you increase the prefix length? Does the random\ntext make more sense? Once your program is working, you might want to try a mash-up:\nif you combine text from two or more books, the random\ntext you generate will blend the vocabulary and phrases from\nthe sources in interesting ways. Credit: This case study is based on an example from Kernighan and\nPike, The Practice of Programming , Addison-Wesley, 1999. You should attempt this exercise before you go on; then you can\ndownload my solution from https://thinkpython.com/code/markov.py .\nYou will also need https://thinkpython.com/code/emma.txt . 13.9  Data structures Using Markov analysis to generate random text is fun, but there is\nalso a point to this exercise: data structure selection. In your\nsolution to the previous exercises, you had to choose: How to represent the prefixes. How to represent the collection of possible suffixes. How to represent the mapping from each prefix to\nthe collection of possible suffixes. The last one is easy: a dictionary is the obvious choice\nfor a mapping from keys to corresponding values. For the prefixes, the most obvious options are string,\nlist of strings, or tuple of strings. For the suffixes,\none option is a list; another is a histogram (dictionary). How should you choose? The first step is to think about\nthe operations you will need to implement for each data structure.\nFor the prefixes, we need to be able to remove words from\nthe beginning and add to the end. For example, if the current\nprefix is “Half a”, and the next word is “bee”, you need\nto be able to form the next prefix, “a bee”. Your first choice might be a list, since it is easy to add\nand remove elements, but we also need to be able to use the\nprefixes as keys in a dictionary, so that rules out lists.\nWith tuples, you can’t append or remove, but you can use\nthe addition operator to form a new tuple: def shift(prefix, word):\n    return prefix[1:] + (word,) shift takes a tuple of words, prefix , and a string, word , and forms a new tuple that has all the words\nin prefix except the first, and word added to\nthe end. For the collection of suffixes, the operations we need to\nperform include adding a new suffix (or increasing the frequency\nof an existing one), and choosing a random suffix. Adding a new suffix is equally easy for the list implementation\nor the histogram. Choosing a random element from a list\nis easy; choosing from a histogram is harder to do\nefficiently (see Exercise 7 ). So far we have been talking mostly about ease of implementation,\nbut there are other factors to consider in choosing data structures.\nOne is run time. Sometimes there is a theoretical reason to expect\none data structure to be faster than other; for example, I mentioned\nthat the in operator is faster for dictionaries than for lists,\nat least when the number of elements is large. But often you don’t know ahead of time which implementation will\nbe faster. One option is to implement both of them and see which\nis better. This approach is called benchmarking . A practical\nalternative is to choose the data structure that is\neasiest to implement, and then see if it is fast enough for the\nintended application. If so, there is no need to go on. If not,\nthere are tools, like the profile module, that can identify\nthe places in a program that take the most time. The other factor to consider is storage space. For example, using a\nhistogram for the collection of suffixes might take less space because\nyou only have to store each word once, no matter how many times it\nappears in the text. In some cases, saving space can also make your\nprogram run faster, and in the extreme, your program might not run at\nall if you run out of memory. But for many applications, space is a\nsecondary consideration after run time. One final thought: in this discussion, I have implied that\nwe should use one data structure for both analysis and generation. But\nsince these are separate phases, it would also be possible to use one\nstructure for analysis and then convert to another structure for\ngeneration. This would be a net win if the time saved during\ngeneration exceeded the time spent in conversion. 13.10  Debugging When you are debugging a program, and especially if you are\nworking on a hard bug, there are five things to try: Reading: Examine your code, read it back to yourself, and\ncheck that it says what you meant to say. Running: Experiment by making changes and running different\nversions. Often if you display the right thing at the right place\nin the program, the problem becomes obvious, but sometimes you have to\nbuild scaffolding. Ruminating: Take some time to think! What kind of error\nis it: syntax, runtime, or semantic? What information can you get from\nthe error messages, or from the output of the program? What kind of\nerror could cause the problem you’re seeing? What did you change\nlast, before the problem appeared? Rubberducking: If you explain the problem to someone else, you\nsometimes find the answer before you finish asking the question.\nOften you don’t need the other person; you could just talk to a rubber\nduck. And that’s the origin of the well-known strategy called rubber duck debugging . I am not making this up; see https://en.wikipedia.org/wiki/Rubber_duck_debugging . Retreating: At some point, the best thing to do is back\noff, undoing recent changes, until you get back to a program that\nworks and that you understand. Then you can start rebuilding. Beginning programmers sometimes get stuck on one of these activities\nand forget the others. Each activity comes with its own failure\nmode. For example, reading your code might help if the problem is a\ntypographical error, but not if the problem is a conceptual\nmisunderstanding. If you don’t understand what your program does, you\ncan read it 100 times and never see the error, because the error is in\nyour head. Running experiments can help, especially if you run small, simple\ntests. But if you run experiments without thinking or reading your\ncode, you might fall into a pattern I call “random walk programming”,\nwhich is the process of making random changes until the program\ndoes the right thing. Needless to say, random walk programming\ncan take a long time. You have to take time to think. Debugging is like an\nexperimental science. You should have at least one hypothesis about\nwhat the problem is. If there are two or more possibilities, try to\nthink of a test that would eliminate one of them. But even the best debugging techniques will fail if there are too many\nerrors, or if the code you are trying to fix is too big and\ncomplicated. Sometimes the best option is to retreat, simplifying the\nprogram until you get to something that works and that you\nunderstand. Beginning programmers are often reluctant to retreat because\nthey can’t stand to delete a line of code (even if it’s wrong).\nIf it makes you feel better, copy your program into another file\nbefore you start stripping it down. Then you can copy the pieces\nback one at a time. Finding a hard bug requires reading, running, ruminating, and\nsometimes retreating. If you get stuck on one of these activities,\ntry the others. 13.11  Glossary deterministic: Pertaining to a program that does the same\nthing each time it runs, given the same inputs. pseudorandom: Pertaining to a sequence of numbers that appears\nto be random, but is generated by a deterministic program. default value: The value given to an optional parameter if no\nargument is provided. override: To replace a default value with an argument. benchmarking: The process of choosing between data structures\nby implementing alternatives and testing them on a sample of the\npossible inputs. rubber duck debugging: Debugging by explaining your problem\nto an inanimate object such as a rubber duck. Articulating the\nproblem can help you solve it, even if the rubber duck doesn’t know\nPython. 13.12  Exercises Exercise 9 The “rank” of a word is its position in a list of words\nsorted by frequency: the most common word has rank 1, the\nsecond most common has rank 2, etc. Zipf’s law describes a relationship between the ranks and frequencies\nof words in natural languages\n( http://en.wikipedia.org/wiki/Zipf's_law ). Specifically, it\npredicts that the frequency, f , of the word with rank r is: f = c r − s where s and c are parameters that depend on the language and the\ntext. If you take the logarithm of both sides of this equation, you\nget: log f = log c − s log r So if you plot log f versus log r , you should get\na straight line with slope − s and intercept log c . Write a program that reads a text from a file, counts\nword frequencies, and prints one line\nfor each word, in descending order of frequency, with\nlog f and log r . Use the graphing program of your\nchoice to plot the results and check whether they form\na straight line. Can you estimate the value of s ? Solution: https://thinkpython.com/code/zipf.py .\nTo run my solution, you need the plotting module matplotlib .\nIf you installed Anaconda, you already have matplotlib ;\notherwise you might have to install it. Buy this book at Amazon.com\n\n#### Contribute\n\nIf you would like to make a contribution to support my books,\nyou can use the button below and pay with PayPal.  Thank you!\nPay what you want:\nSmall $1.00 USD\nMedium $5.00 USD\nLarge $10.00 USD\nX-Large $20.00 USD\nXX-Large $50.00 USD\nAre you using one of our books in a class? We'd like to know\nabout it.  Please consider filling out this short survey . Think DSP Think Java Think Bayes Think Python 2e Think Stats 2e Think Complexity",
  "type": "tutorial",
  "concept_tags": [
    "OOP",
    "special_methods",
    "operator_overloading"
  ],
  "difficulty": "intermediate",
  "metadata": {
    "language": "en",
    "domain": "programming/python",
    "retrieved_at": "2026-02-24",
    "intended_use": "educational corpus",
    "source": "think_python"
  },
  "provenance": {
    "url": "https://greenteapress.com/thinkpython2/html/thinkpython2014.html",
    "license": "CC BY-NC 3.0",
    "attribution": "Allen B. Downey, greenteapress.com",
    "retrieved_at": "2026-02-24"
  },
  "ai_generated": false,
  "doc_id": "scraped-https-greenteapress-com-thinkpython2-html-thinkpython2014-ht-8ba08b25"
}