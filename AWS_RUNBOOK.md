# AdapTeach Benchmark — AWS Replicability Runbook
### Full ground-up setup from a fresh git clone

---

## What This Runbook Does

Reproduces the full AdapTeach benchmark pipeline on a clean Linux machine:
- **Objective 1** — Retrieval quality benchmarks (nDCG@10, MRR@10, context precision) across 6 configs × 4 datasets, with automated relevance labeling
- **Objective 2** — Generation quality via RAGAS (faithfulness, answer relevancy, context precision, context recall) using Gemini 2.5 Flash

**Gemini model:** `gemini-2.5-flash` (free tier, 10 RPM · 250 RPD)

---

## What Is and Isn't in the Repo

| What | Tracked? | Notes |
|---|---|---|
| Source code (`src/`, `bench/`, `configs/`) | ✅ Yes | |
| Corpus docs (`data/corpus_raw/`, `data/corpus_clean/`) | ✅ Yes | 59 documents |
| Corpus snapshot (`data/snapshots/`) | ✅ Yes | Use existing snapshot ID |
| Knowledge graphs (`graphs/ckg.json`, `graphs/cpg.json`) | ✅ Yes | Pre-built |
| Query files (`bench/queries_*.jsonl`) | ✅ Yes | All 4 datasets |
| Final results (`bench/results/`) | ✅ Yes | For reference |
| **Vector indexes (`indexes/`)** | ❌ No | Must rebuild — step 4 |
| Run outputs (`bench/runs/*.jsonl`) | ❌ No | Generated by you |
| Labels (`bench/labels/*.csv`) | ❌ No | Generated by you |
| Golden answers (`bench/golden_*.jsonl`) | ❌ No | Generated by you |

---

## Part 0 — AWS Instance Setup

**Recommended instance:** `t3.large` (2 vCPU, 8 GB RAM, 20 GB gp3 EBS)
**AMI:** Ubuntu 22.04 LTS

### 0.1 SSH into your instance

```bash
ssh -i ~/your-key.pem ubuntu@<YOUR_AWS_IP>
```

### 0.2 Install system dependencies

```bash
sudo apt update && sudo apt install -y \
  python3-pip python3-venv git

# Verify Python version (need 3.10+)
python3 --version
```

---

## Part 1 — Clone and Install

### 1.1 Clone the repo

```bash
git clone https://github.com/<your-username>/benchmark.git
cd benchmark
```

### 1.2 Create virtual environment and install dependencies

```bash
python3 -m venv .venv
source .venv/bin/activate

pip install --upgrade pip
pip install -r requirements.lock

# Objective 2 extras (RAGAS + Gemini)
pip install "ragas==0.4.3" langchain-google-genai langchain-core google-generativeai
```

### 1.3 Set environment variables

```bash
export GEMINI_API_KEY=<YOUR_GEMINI_API_KEY>
export TF_ENABLE_ONEDNN_OPTS=0
export PYTHONPATH=$(pwd)
```

Persist across sessions:

```bash
echo "export GEMINI_API_KEY=<YOUR_GEMINI_API_KEY>" >> ~/.bashrc
echo "export TF_ENABLE_ONEDNN_OPTS=0" >> ~/.bashrc
echo "export PYTHONPATH=~/benchmark" >> ~/.bashrc
source ~/.bashrc
```

---

## Part 2 — Rebuild Vector Index

The corpus (59 docs) and knowledge graphs are already in the repo.
The vector index is NOT tracked (too large). Rebuild it now.

### 2.1 Verify corpus and snapshot exist

```bash
# Should show 59 JSON files
ls data/corpus_clean/ | wc -l

# Should show an existing snapshot ID
ls data/snapshots/
# Example output: 1538fae68752ebed
```

### 2.2 Build chunks

```bash
python -m src.chunking.build_chunks \
  --chunker ast \
  --chunk-size 450 \
  --overlap 80
```

Expected output: ~1,460 chunks (code + text). Verify:

```bash
python -c "
import json
d = json.load(open('data/corpus_meta/chunk_stats_report.json'))
print(f'Total: {d[\"chunk_count\"]}  Code: {d[\"content_type_counts\"].get(\"code\",0)}  Text: {d[\"content_type_counts\"].get(\"text\",0)}')
"
```

### 2.3 Build vector index

```bash
# Use the snapshot ID shown by `ls data/snapshots/`
SNAPSHOT_ID=$(ls data/snapshots/ | head -1)
echo "Using snapshot: $SNAPSHOT_ID"

python -m src.indexing.build_indexes \
  --snapshot-id $SNAPSHOT_ID \
  --embedder-version minilm-v2
```

This prints a new `index_id` at the end. Copy it — you need it in the next step.
Example output: `index_id: 760e73c3...`

### 2.4 Update config.yaml with the new index

```bash
# Grab the index_id printed by the previous command
INDEX_ID=<PASTE_INDEX_ID_HERE>

# Update config.yaml
sed -i "s|index_hash:.*|index_hash: $INDEX_ID|" config.yaml
sed -i "s|corpus_snapshot:.*|corpus_snapshot: $SNAPSHOT_ID|" config.yaml

# Verify
grep -E "index_hash|corpus_snapshot" config.yaml
```

### 2.5 Verify retrieval works

```bash
python -c "
from src.pipelines.runner import run_pipeline
from src.utils.config import load_app_config, load_pipeline_config
request = {'query': 'How do for loops work in Python?', 'artifact_type': 'flashcard',
           'query_id': 'test-1', 'category': '', 'topic': ''}
response, _ = run_pipeline(request=request, app_config=load_app_config(),
                           pipeline_config=load_pipeline_config('B'), dry_run='retrieval')
ctx = response.get('context', [])
print(f'Chunks returned: {len(ctx)}')
for c in ctx[:3]:
    title = c.get('provenance', {}).get('title', c.get('chunk_id', '?'))
    print(f'  [{c.get(\"score\", 0):.3f}] {title}')
" 2>/dev/null
```

Expected: 3–10 chunks with non-zero scores and titles related to loops/iteration.
If you get 0 chunks or a `KeyError: index_hash`, recheck the `config.yaml` values from step 2.4.

---

## Part 3 — Objective 1: Retrieval Benchmarks

**Goal:** Compare 6 pipeline configs across 4 query datasets using retrieval-only (dry-run) mode.
No LLM calls. No API quota used. Fast.

### Pipeline overview

```
Query files ──► run_benchmark_suite.py (--dry-run retrieval)
             ──► bench/runs/run_TIMESTAMP_TAG.jsonl
             ──► export_labeling_sheet.py
             ──► bench/labels/sheet_TAG.csv
             ──► auto_label.py
             ──► bench/labels/labels_TAG.csv
             ──► compute_metrics.py
             ──► bench/results/metrics_TAG.csv
```

### 3.1 Run retrieval benchmark — all 4 datasets

```bash
cd ~/benchmark
source .venv/bin/activate

for TAG in custom cs1qa mbpp staqc; do
  echo "=== Running: $TAG ==="
  python bench/run_benchmark_suite.py \
    --query-set bench/queries_${TAG}.jsonl \
    --configs A,B,C,D,E,F \
    --dry-run retrieval \
    --tag $TAG
done
```

Each dataset takes ~5–15 min. Total: ~40–60 min for all 4.
Use tmux to keep it running if you close SSH:

```bash
tmux new-session -d -s obj1
tmux send-keys -t obj1 "
source .venv/bin/activate &&
for TAG in custom cs1qa mbpp staqc; do
  echo '=== Running: '$TAG' ===' &&
  python bench/run_benchmark_suite.py --query-set bench/queries_\${TAG}.jsonl --configs A,B,C,D,E,F --dry-run retrieval --tag \$TAG
done
" Enter
# Detach: ctrl+b d
# Reattach: tmux attach -t obj1
```

Verify all 4 runs completed:
```bash
ls bench/runs/run_*.jsonl
# Should see 4 files, one per tag
```

---

### 3.2 Auto-label all datasets

Auto-labeling assigns a relevance score (0/1/2) to each retrieved (query, chunk) pair
using **Gemini 2.5 Flash as an LLM judge** — more accurate than cosine similarity alone.

Pairs are sent in batches of 20 per API call. Labeling is **resumable** — if the script
is interrupted, re-running it skips already-labeled pairs.

**Free-tier call budget (10 RPM / 250 RPD):**

| Dataset | ~Pairs | ~Batches (sz=20) | Est. time |
|---|---|---|---|
| custom | ~1,400 | ~70 | ~9 min |
| cs1qa | ~5,200 | ~260 | ~2 days |
| mbpp | ~6,000 | ~300 | ~2 days |
| staqc | ~6,000 | ~300 | ~2 days |

Run `custom` first (fits in one session). Run the others one dataset per day.

```bash
# Step 1: export labeling sheets for all datasets (fast, no API)
for TAG in custom cs1qa mbpp staqc; do
  RUNFILE=$(ls -t bench/runs/run_*_${TAG}.jsonl | grep -v "custom_full" | head -1)
  echo "=== Exporting sheet: $TAG ==="
  python bench/export_labeling_sheet.py \
    --run-file "$RUNFILE" \
    --configs B,C,D,E,F \
    --out-csv bench/labels/sheet_${TAG}.csv
done

# Step 2: auto-label with Gemini (run one dataset per day on free tier)
for TAG in custom; do   # extend to cs1qa mbpp staqc on subsequent days
  echo "=== Labeling: $TAG ==="
  python bench/auto_label.py \
    --gemini \
    --sheet bench/labels/sheet_${TAG}.csv \
    --queries bench/queries_${TAG}.jsonl \
    --out bench/labels/labels_${TAG}.csv \
    --api-key $GEMINI_API_KEY \
    --batch-size 20 \
    --delay 7
  echo "Done: bench/labels/labels_${TAG}.csv"
done
```

Run inside tmux for large datasets (cs1qa/mbpp/staqc):
```bash
tmux new-session -d -s label_cs1qa
tmux send-keys -t label_cs1qa "
cd ~/adapteach-benchmark && source .venv/bin/activate &&
python bench/auto_label.py --gemini \
  --sheet bench/labels/sheet_cs1qa.csv \
  --queries bench/queries_cs1qa.jsonl \
  --out bench/labels/labels_cs1qa.csv \
  --api-key \$GEMINI_API_KEY --batch-size 20 --delay 7
" Enter
# ctrl+b d to detach; tmux attach -t label_cs1qa to check progress
```

---

### 3.3 Compute metrics per dataset

`--no-combined` suppresses the redundant "combined" row in each per-dataset file
(the merge step in 3.4 handles the combined aggregation instead).

```bash
for TAG in custom cs1qa mbpp staqc; do
  RUNFILE=$(ls -t bench/runs/run_*_${TAG}.jsonl | grep -v "custom_full" | head -1)

  python bench/compute_metrics.py \
    --run-file "$RUNFILE" \
    --labels-csv bench/labels/labels_${TAG}.csv \
    --out-csv bench/results/metrics_${TAG}.csv \
    --no-combined

  echo "Done: bench/results/metrics_${TAG}.csv"
done
```

---

### 3.4 Merge all datasets into one results table

`compute_metrics.py` already writes a `dataset` column, so the merge just
concatenates the CSVs directly — no column duplication.

```bash
python -c "
import csv, pathlib

datasets = ['custom', 'cs1qa', 'mbpp', 'staqc']
all_rows, header = [], None

for tag in datasets:
    p = pathlib.Path(f'bench/results/metrics_{tag}.csv')
    if not p.exists():
        print(f'Missing: {p}'); continue
    rows = list(csv.DictReader(open(p, encoding='utf-8')))
    if not header:
        header = list(rows[0].keys())
    all_rows.extend(rows)

out = pathlib.Path('bench/results/obj1_all_datasets.csv')
with open(out, 'w', newline='', encoding='utf-8') as f:
    w = csv.DictWriter(f, fieldnames=header)
    w.writeheader()
    w.writerows(all_rows)
print(f'Merged {len(all_rows)} rows to {out}')
"
```

### 3.5 Generate figure

```bash
python bench/plot_obj1.py \
  --csv bench/results/obj1_all_datasets.csv \
  --out bench/figures/fig2_obj1_by_dataset.png

echo "Figure saved: bench/figures/fig2_obj1_by_dataset.png"
```

---

## Part 4 — Objective 2: RAGAS Generation Quality

**Goal:** Evaluate artifact generation quality via RAGAS for top retrieval configs.
**Uses Gemini API — free tier (250 RPD, 10 RPM). Plan for 3 days.**

### Free tier call budget

| Day | Task | Calls |
|---|---|---|
| Day 1 | Pipeline run — 30 queries × 4 configs (B,D,E,F) | 120 |
| Day 1 | Build golden reference answers — 30 queries | 30 |
| Day 2 | RAGAS eval — configs B, D (30 × 2 × 4 metrics) | 240 |
| Day 3 | RAGAS eval — configs E, F (30 × 2 × 4 metrics) | 240 |
| **Total** | | **630 calls / ~3 days** |

### 4.1 Full pipeline run (Day 1)

Gemini generates an artifact answer for each query. `--delay 7` keeps calls at ~8 RPM.

```bash
tmux new-session -d -s obj2
tmux send-keys -t obj2 "
cd ~/benchmark && source .venv/bin/activate &&
python bench/run_benchmark_suite.py \
  --query-set bench/queries_custom.jsonl \
  --configs B,D,E,F \
  --provider gemini \
  --model gemini-2.5-flash \
  --sample 30 \
  --delay 7 \
  --tag custom_full
" Enter
tmux attach -t obj2
# ctrl+b d to detach
```

Takes ~35 min (30 queries × 4 configs × 7s delay).

Verify it completed:
```bash
RUNFILE=$(ls -t bench/runs/run_*_custom_full.jsonl | head -1)
python -c "
import json
rows = [json.loads(l) for l in open('$RUNFILE') if l.strip()]
cfgs = sorted(set(r['config'] for r in rows))
print(f'Rows: {len(rows)}, Configs: {cfgs}')
"
# Expected: Rows: 120, Configs: ['B', 'D', 'E', 'F']
```

---

### 4.2 Build golden reference answers (Day 1, after pipeline run)

```bash
RUNFILE=$(ls -t bench/runs/run_*_custom_full.jsonl | head -1)

python bench/ragas_eval.py \
  --build-golden \
  --queries bench/queries_custom.jsonl \
  --run-file "$RUNFILE" \
  --out bench/golden_custom.jsonl \
  --delay 7 \
  --api-key $GEMINI_API_KEY
```

30 queries × 7s delay ≈ 4 min. The script resumes if interrupted — already-done entries are skipped.

Spot-check the output:
```bash
head -2 bench/golden_custom.jsonl | python -m json.tool
# Each entry should have: id, query, ground_truth (a real 2-3 sentence answer)
```

---

### 4.3 RAGAS evaluation — configs B and D (Day 2)

```bash
RUNFILE=$(ls -t bench/runs/run_*_custom_full.jsonl | head -1)

tmux new-session -d -s ragas_bd
tmux send-keys -t ragas_bd "
cd ~/benchmark && source .venv/bin/activate &&
python bench/ragas_eval.py \
  --eval \
  --run-file $RUNFILE \
  --golden bench/golden_custom.jsonl \
  --configs B D \
  --out bench/results/ragas_BD.csv \
  --api-key \$GEMINI_API_KEY
" Enter
tmux attach -t ragas_bd
```

~240 Gemini calls. RAGAS handles rate limits with automatic exponential backoff retries.
Takes ~30–60 min.

---

### 4.4 RAGAS evaluation — configs E and F (Day 3)

```bash
RUNFILE=$(ls -t bench/runs/run_*_custom_full.jsonl | head -1)

tmux new-session -d -s ragas_ef
tmux send-keys -t ragas_ef "
cd ~/benchmark && source .venv/bin/activate &&
python bench/ragas_eval.py \
  --eval \
  --run-file $RUNFILE \
  --golden bench/golden_custom.jsonl \
  --configs E F \
  --out bench/results/ragas_EF.csv \
  --api-key \$GEMINI_API_KEY
" Enter
tmux attach -t ragas_ef
```

---

### 4.5 Merge RAGAS results

```bash
python -c "
import csv, pathlib

all_rows, header = [], None
for f in ['bench/results/ragas_BD.csv', 'bench/results/ragas_EF.csv']:
    p = pathlib.Path(f)
    if not p.exists():
        print(f'Missing: {p}'); continue
    rows = list(csv.DictReader(open(p, encoding='utf-8')))
    if not header:
        header = list(rows[0].keys())
    all_rows.extend(rows)

out = pathlib.Path('bench/results/ragas_custom.csv')
with open(out, 'w', newline='', encoding='utf-8') as f:
    w = csv.DictWriter(f, fieldnames=header)
    w.writeheader()
    w.writerows(all_rows)
print(f'Merged to {out}')
print()
for r in sorted(all_rows, key=lambda x: x['config']):
    print(f\"Config {r['config']}: faithfulness={r.get('faithfulness','?')}  answer_relevancy={r.get('answer_relevancy','?')}  ctx_precision={r.get('context_precision','?')}  ctx_recall={r.get('context_recall','?')}\")
"
```

---

## Part 5 — Transfer Results Back

From your local machine (Windows Git Bash):

```bash
# Pull only the results and figures (small)
rsync -avz \
  -e "ssh -i ~/your-key.pem" \
  ubuntu@<AWS_IP>:~/benchmark/bench/results/ \
  /c/Users/Nokie/Desktop/Thesis_Proj/benchmark/bench/results/

rsync -avz \
  -e "ssh -i ~/your-key.pem" \
  ubuntu@<AWS_IP>:~/benchmark/bench/figures/ \
  /c/Users/Nokie/Desktop/Thesis_Proj/benchmark/bench/figures/
```

---

## Troubleshooting

**`ModuleNotFoundError: No module named 'src'`**
```bash
export PYTHONPATH=~/benchmark
# or run: cd ~/benchmark first
```

**Retrieval returns 0 chunks / index not found**
```bash
grep "index_hash" config.yaml
ls indexes/   # Should show one directory matching the hash
```
If `indexes/` is empty, re-run Part 2 (rebuild index).

**Gemini 429 rate limit error in pipeline run**
→ Increase `--delay 10` or `--delay 15`. Free tier allows bursts but not sustained high RPM.

**RAGAS returns all NaN for context_precision / context_recall**
→ `ground_truth` missing in golden file. Check: `head -1 bench/golden_custom.jsonl | python -m json.tool`
→ The field must be named `ground_truth`, not `reference`.

**`bench/runs/run_*_custom_full.jsonl` not found**
→ Pipeline run didn't complete. Check tmux session: `tmux attach -t obj2`

**RAGAS crashes halfway**
→ Results are NOT resumable mid-config. Re-run the same `--configs` flags; it will overwrite `ragas_BD.csv`.

**`ls data/snapshots/` is empty**
```bash
# Recreate snapshot from existing clean corpus
python -m src.indexing.snapshot_tool create
SNAPSHOT_ID=$(ls data/snapshots/ | head -1)
# Then re-run Part 2.3 with this SNAPSHOT_ID
```

---

## Expected Final Outputs

```
bench/results/
  obj1_all_datasets.csv   ← Obj 1 retrieval metrics (nDCG@10, MRR@10 per dataset per config)
  ragas_custom.csv        ← Obj 2 generation metrics (faithfulness etc. per config)

bench/figures/
  fig2_obj1_by_dataset.png  ← 2×2 faceted retrieval quality figure
```
